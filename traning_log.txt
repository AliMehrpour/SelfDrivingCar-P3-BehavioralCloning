Batch generating for train set... Number of data:21932
Epoch 1/50
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.91GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
Reshuffling train set...
/home/carnd/anaconda3/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.
  warnings.warn('Epoch comprised more than '
Batch generating for validation set... Number of data:5483
Reshuffling validation set...
82s - loss: 0.0599 - acc: 0.2338 - val_loss: 0.0436 - val_acc: 0.2369
Epoch 2/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0426 - acc: 0.2326 - val_loss: 0.0430 - val_acc: 0.2356
Epoch 3/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0396 - acc: 0.2326 - val_loss: 0.0385 - val_acc: 0.2360
Epoch 4/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0389 - acc: 0.2328 - val_loss: 0.0412 - val_acc: 0.2360
Epoch 5/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0378 - acc: 0.2327 - val_loss: 0.0422 - val_acc: 0.2360
Epoch 6/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0380 - acc: 0.2324 - val_loss: 0.0322 - val_acc: 0.2333
Epoch 7/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0349 - acc: 0.2315 - val_loss: 0.0338 - val_acc: 0.2351
Epoch 8/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0360 - acc: 0.2318 - val_loss: 0.0372 - val_acc: 0.2364
Epoch 9/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0356 - acc: 0.2329 - val_loss: 0.0380 - val_acc: 0.2349
Epoch 10/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0348 - acc: 0.2315 - val_loss: 0.0355 - val_acc: 0.2355
Epoch 11/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0345 - acc: 0.2331 - val_loss: 0.0342 - val_acc: 0.2371
Epoch 12/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0346 - acc: 0.2313 - val_loss: 0.0371 - val_acc: 0.2411
Epoch 13/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0353 - acc: 0.2330 - val_loss: 0.0317 - val_acc: 0.2373
Epoch 14/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0340 - acc: 0.2329 - val_loss: 0.0327 - val_acc: 0.2344
Epoch 15/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0335 - acc: 0.2303 - val_loss: 0.0337 - val_acc: 0.2338
Epoch 16/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0333 - acc: 0.2329 - val_loss: 0.0340 - val_acc: 0.2420
Epoch 17/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0332 - acc: 0.2331 - val_loss: 0.0338 - val_acc: 0.2342
Epoch 18/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0331 - acc: 0.2316 - val_loss: 0.0301 - val_acc: 0.2360
Epoch 19/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0323 - acc: 0.2306 - val_loss: 0.0312 - val_acc: 0.2322
Epoch 20/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0346 - acc: 0.2324 - val_loss: 0.0365 - val_acc: 0.2393
Epoch 21/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0327 - acc: 0.2326 - val_loss: 0.0336 - val_acc: 0.2351
Epoch 22/50
Reshuffling train set...
Reshuffling validation set...
Reshuffling validation set...
77s - loss: 0.0334 - acc: 0.2331 - val_loss: 0.0337 - val_acc: 0.2356
Epoch 23/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0326 - acc: 0.2335 - val_loss: 0.0317 - val_acc: 0.2346
Epoch 24/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0320 - acc: 0.2299 - val_loss: 0.0386 - val_acc: 0.2364
Epoch 25/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0329 - acc: 0.2328 - val_loss: 0.0312 - val_acc: 0.2331
Epoch 26/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0334 - acc: 0.2315 - val_loss: 0.0344 - val_acc: 0.2391
Epoch 27/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0326 - acc: 0.2323 - val_loss: 0.0384 - val_acc: 0.2336
Epoch 28/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0319 - acc: 0.2325 - val_loss: 0.0332 - val_acc: 0.2396
Epoch 29/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0342 - acc: 0.2322 - val_loss: 0.0353 - val_acc: 0.2362
Epoch 30/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0331 - acc: 0.2293 - val_loss: 0.0351 - val_acc: 0.2391
Epoch 31/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0322 - acc: 0.2331 - val_loss: 0.0308 - val_acc: 0.2346
Epoch 32/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0303 - acc: 0.2325 - val_loss: 0.0312 - val_acc: 0.2347
Epoch 33/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0343 - acc: 0.2326 - val_loss: 0.0316 - val_acc: 0.2420
Epoch 34/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0312 - acc: 0.2332 - val_loss: 0.0332 - val_acc: 0.2346
Epoch 35/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0312 - acc: 0.2290 - val_loss: 0.0316 - val_acc: 0.2413
Epoch 36/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0296 - acc: 0.2360 - val_loss: 0.0276 - val_acc: 0.2340
Epoch 37/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0316 - acc: 0.2337 - val_loss: 0.0333 - val_acc: 0.2353
Epoch 38/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0309 - acc: 0.2295 - val_loss: 0.0367 - val_acc: 0.2386
Epoch 39/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0311 - acc: 0.2354 - val_loss: 0.0353 - val_acc: 0.2347
Epoch 40/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0296 - acc: 0.2299 - val_loss: 0.0323 - val_acc: 0.2336
Epoch 41/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0310 - acc: 0.2332 - val_loss: 0.0338 - val_acc: 0.2416
Epoch 42/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0311 - acc: 0.2283 - val_loss: 0.0339 - val_acc: 0.2353
Epoch 43/50
Reshuffling train set...
Reshuffling validation set...
Reshuffling validation set...
77s - loss: 0.0322 - acc: 0.2335 - val_loss: 0.0305 - val_acc: 0.2351
Epoch 44/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0309 - acc: 0.2318 - val_loss: 0.0311 - val_acc: 0.2351
Epoch 45/50
Reshuffling train set...
Reshuffling validation set...
78s - loss: 0.0290 - acc: 0.2330 - val_loss: 0.0309 - val_acc: 0.2331
Epoch 46/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0311 - acc: 0.2326 - val_loss: 0.0317 - val_acc: 0.2426
Epoch 47/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0311 - acc: 0.2333 - val_loss: 0.0321 - val_acc: 0.2340
Epoch 48/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0307 - acc: 0.2307 - val_loss: 0.0333 - val_acc: 0.2378
Epoch 49/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0304 - acc: 0.2303 - val_loss: 0.0305 - val_acc: 0.2346
Epoch 50/50
Reshuffling train set...
Reshuffling validation set...
77s - loss: 0.0327 - acc: 0.2336 - val_loss: 0.0303 - val_acc: 0.2371
{'val_loss': [0.043567740813244223, 0.043035498235461322, 0.038494436590130937, 0.041168633545207424, 0.042195063041046608, 0.032155626540093921, 0.033795077808547853, 0.037188764878137166, 0.038040195656723758, 0.035507852601450542, 0.034220273433209852, 0.037130804775759231, 0.031719086431833202, 0.032660573222782721, 0.033726237064530686, 0.033995834785665188, 0.033790864526878954, 0.030061309788982537, 0.031176086125332257, 0.03645926038193148, 0.033618609291003194, 0.033664896131254902, 0.031735504046082497, 0.038577414355998818, 0.031246684578269027, 0.034365273535598158, 0.038390800183595614, 0.03315338182674591, 0.035289931500893691, 0.035092716278551622, 0.030751938953302627, 0.031214672638926397, 0.03157803017732709, 0.033231189149583493, 0.031638612494219183, 0.027590509222517181, 0.033305636758721155, 0.036744707588886108, 0.035281950802823835, 0.032315807265424448, 0.033777581094655881, 0.033887522879901322, 0.030496480874717236, 0.031127285212278366, 0.030910600474927316, 0.031706655614597852, 0.032128005613421287, 0.033310227226032767, 0.030514389709677806, 0.030304870698167834], 'acc': [0.2337845203488372, 0.23260356104651161, 0.23260356104651161, 0.23278524709302326, 0.23269440406976744, 0.2323764534883721, 0.23151344476744187, 0.2318313953488372, 0.23287609011627908, 0.23151344476744187, 0.23310319767441862, 0.23133175872093023, 0.2330123546511628, 0.23287609011627908, 0.23028706395348839, 0.23292151162790697, 0.23305777616279069, 0.23164970930232559, 0.23055959302325582, 0.232421875, 0.23264898255813954, 0.23305777616279069, 0.23351199127906977, 0.2298782703488372, 0.23278524709302326, 0.23151344476744187, 0.23233103197674418, 0.2324672965116279, 0.23224018895348839, 0.22933321220930233, 0.23314861918604651, 0.23251271802325582, 0.23260356104651161, 0.23319404069767441, 0.22896984011627908, 0.23596475290697674, 0.23373909883720931, 0.22946947674418605, 0.23541969476744187, 0.22992369186046513, 0.23319404069767441, 0.22828851744186046, 0.23351199127906977, 0.23178597383720931, 0.23296693313953487, 0.23264898255813954, 0.23333030523255813, 0.23069585755813954, 0.23033248546511628, 0.23360283430232559], 'val_acc': [0.2369186046511628, 0.23564680232558138, 0.23601017441860464, 0.23601017441860464, 0.23601017441860464, 0.23328488372093023, 0.23510174418604651, 0.2363735465116279, 0.23492005813953487, 0.23546511627906977, 0.23710029069767441, 0.24109738372093023, 0.23728197674418605, 0.234375, 0.23382994186046513, 0.24200581395348839, 0.23419331395348839, 0.23601017441860464, 0.23219476744186046, 0.23928052325581395, 0.23510174418604651, 0.23564680232558138, 0.23455668604651161, 0.2363735465116279, 0.23310319767441862, 0.23909883720930233, 0.23364825581395349, 0.2396438953488372, 0.23619186046511628, 0.23909883720930233, 0.23455668604651161, 0.23473837209302326, 0.24200581395348839, 0.23455668604651161, 0.24127906976744187, 0.23401162790697674, 0.23528343023255813, 0.23855377906976744, 0.23473837209302326, 0.23364825581395349, 0.24164244186046513, 0.23528343023255813, 0.23510174418604651, 0.23510174418604651, 0.23310319767441862, 0.24255087209302326, 0.23401162790697674, 0.23782703488372092, 0.23455668604651161, 0.23710029069767441], 'loss': [0.059864957708605497, 0.042550579991278259, 0.039629078919587786, 0.038870831242177724, 0.037751744403828717, 0.037964253778417792, 0.034853741391237043, 0.035960958149760615, 0.035617272716102211, 0.034762197569402499, 0.034509415711211258, 0.034562731936010858, 0.035274831523988831, 0.033983033117946496, 0.033481257583209592, 0.033301675784249987, 0.033164549819302072, 0.0331402066336988, 0.032264255913147744, 0.034607668710482674, 0.032686916266589663, 0.033380257593857689, 0.032589938930170828, 0.031993316345696535, 0.032892653009269475, 0.033430727539843948, 0.032565033436904464, 0.031901897147817666, 0.034210346314276371, 0.033098815566708531, 0.032178050129089593, 0.030302212167487935, 0.034342382184424716, 0.031220792522004177, 0.031208134689476599, 0.02957558124542756, 0.03158169834831253, 0.0308906982522891, 0.031070397820237072, 0.029629344066475018, 0.031036966951390684, 0.031124509749176025, 0.032206190019022932, 0.030893889667336329, 0.029016473411777339, 0.03112389909666638, 0.031072272226041139, 0.03071386467795386, 0.030406763121962201, 0.032693476091290627]}